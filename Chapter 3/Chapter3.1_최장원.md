# Linear regression
이 장은 선형 회귀에 관한 내용, 이는 간단한 지도 학습 방법 중 하나이다.  
선형 회귀는 주로 양적인 응답(quantitative response)을 예측하는 데 유용한 도구.  
널리 사용되는 통계적 학습 방법, 이 장에서는 선형 회귀 모델의 기본 아이디어와 이 모델을 가장
흔하게 적합시키는 최소제곱법(least squares)을 검토한다.  
2장에서 살펴본 광고 데이터를 기반으로 마케팅 계획을 제안하라는 요청을 받았다고 가정하면 다루어야 할 몇 가지 질문이 있다.  
1.광고 예산과 매출은 관계가 있나?  
2.광고 예산과 판매량 사이의 관계가 얼마나 강한가?  
3.어떤 미디어가 판매와 관련이 있나?  
4.각 미디어와 판매 간의 관련성은 얼마나 큰가?  
5.미래 판매량을 얼마나 정확하게 예측할 수 있나?  
6.이 관계는 선형인가?  
7.광고 미디어 간에 시너지가 있을까?  
이러한 모든 질문에 선형 회귀를 사용하여 답을 얻을 수 있다.  
## Simple Linear Regression  
단일 예측변수 X를 기반으로 정량적 응답 Y를 예측하기 위한 회귀 접근방식, 이는 대략 선형이 있다고 가정한다. X와 Y사이의 관계: ![Image](./28.png)  (3.1)
예를들어 X는 TV광고, Y는 판매를 나타낼 수 있다.  그럼 우리는 판매량을 TV에 회귀시켜 다음과 같은 모델을 적합시킬 수 있다. ![Image](./29.png)  
식 3.1에서 β0와 β1은 선형 모델의 절편 및 기울기 항을 나타내는 두 개의 알려지지 않은 상수이다. β0와 β1은 모델 계수 또는 매개변수라고 알려져 있다. 훈련 데이터를 사용하여 모델 계수에 대한 추정치 βˆ0 및 βˆ1을 생성한 후, TV 광고의 특정 값을 기반으로 미래 판매량을 예측할 수 있다.  ![Image](./30.png) 여기서 yˆ은 X=x를 기반으로 한 Y의 예측을 나타낸다.  알려지지 않은 매개변수나 계수의 추정값 또는 응답의 예측값을 나타내기 위해 ˆ기호를 사용한다. 
## Estimating the Coefcients  
실제로  β0 와 β1은 알려져 있지 않다. 따라서 예측을 위해서는 데이터를 사용하여 계수를 추정해야 한다. n = 200개의 데이터 포인트에 가능한 가까운 직선을 찾고자 함, 방법은 여러 가지가 있지만, 편차 제곱의 합을 최소화하는 것이 가장 일반적인 방법이다. 대체 방법은 6장에서 고려된다.  yˆi = βˆ0 + βˆ1xi이 i번째 X값에 기반한 Y의 예측이라면, ei = yi −yˆi는 i번째 잔차를 나타내며, 이것은 i번째 관찰된 응답 값과 선형 모델에 의해 예측된 i번째 응답 값 사이의 차이이다. RSS(Residual Sum of Squares)를 다음과 같이 정의한다. ![Image](./31.png) 또는 ![Image](./32.png) 최소 제곱 접근법은 RSS를 최소화하기 위한 βˆ0 및 βˆ1을 선택한다. 일부 미적분학에서는 최소화 프로그램이 다음과 같다는 것을 보여줄 수 있다.  ![Image](./33.png)(3.4) ![Image](./34.png) 광고 데이터에 대한 최소 제곱 선형 회귀 모델을 보여준다. 이 모델은 잔차 합의 최소화를 통해 찾는다. 각 회색 선분은 잔차를 나타낸다. 이 경우 선형 모델이 관계릐 본질을 잘 포착하지만 그림 왼쪽의 경향을 과대 평가한다(왼쪽 부분에서 예측이 과도하게 높게 나타난다).
여기서 βˆ0 = 7.03 이고 βˆ1 = 0.0475이다. 이 근사에 따르면 TV 광고에 추가로 $1000를 투자하면 제품을 약 47.5개 추가 판매가 가능하다. 그림3.2에서는 RSS를 계산했다. 그림에서 빨간 점은 (3.4)로 주어진 최소 제곱 추정치(βˆ0, βˆ1)를 나타낸다. 이 값은 RSS를 최소화한다.  
## Assessing the Accuracy of the Coefcient Estimates(계수 추정의 정확성 평가)  
우리는 알 수 없는 함수 f에 대해 Y = f(X) + ϵ에서 X와 Y 사이의 선형 관계가 참이라고 가정했다. 여기서 엡실론은 평균 0의 무작위 오류 항이다. f를 선형 함수로 근사화하려면 이 관계를 다음과 같이 작성할 수 있다.
![Image](./35.png)(3.5) β0는 절편, β1은 기울기, ϵ은 평균이 0인 오차항이다. 오차 항은 이 간단한 모델로 놓치는 부분을 포괄한다. 일반적으로 오차 항이 X와 독립적이라고 가정한다. ![Image](./36.png)  빨간색 점은 (3.4)에 의해 주어진 최소 제곱 추정치 βˆ0 및 βˆ1에 해당된다. (3.5)에 의해 주어진 모델은 모집단 회귀선을 정의한다. ![Image](./38.png)(3.6) ![Image](./37.png)(3.3)
그림 3.3의 왼쪽 패널에는 간단한 시뮬레이션 예제로 이 두 줄이 표시된다. 100개의 무작위 X를 만들었고, 모델 Y = 2+3X + ϵ에서 100개의 해당 Y를 생성했다.
여기서 ϵ는 평균이 0인 정규 분포에서 생성되었다. 그림 3.3의 왼쪽 패널에서 빨간 선은 실제 관계 f(X) = 2+3X+ϵ를 나타내며, 파란 선은 관측 데이터를 기반으로한 최소 제곱 추정치이다. 실제 데이터에서는 일반적으로 실제 관계를 알 수 없지만, 최소 제곱선은 항상 (3.4)에서 주어진 계수 추정치를 사용하여 계산할 수 있다.  
다시 말해 실제 응용에서는 최소 제곱 선을 계산할 수 있는 관측치 집합에 액세스할 수 있다. 그러나 모집단 회귀선은 관찰할 수 없다.   
즉 "실제 데이터에서는 전체 모집단의 실제 관계를 알 수 없지만, 우리는 실제 데이터를 사용하여 최소 제곱선을 계산하여 실제 데이터에 가장 잘 맞는 선을 얻을 수 있다"로 해석 가능. 그림 3.3의 오른쪽 패널에서는 (3.6)로 주어진 모델에서 생성된 열 가지 다른 데이터 세트를 생성하고 해당 열 가지 최소 제곱선을 그렸다. 동일한 실제 모델에서 생성된 다른 데이터 세트는 약간 다른 최소 제곱선을 결과로 얻지만, 관측되지 않은 모집단 회귀선은 변하지 않는다. 즉, 동일한 실제 모델에서 생성된 다른 데이터 세트는 약간 다른 최소 제곱선을 가지지만, 관측되지 않은 모집단 회귀선은 변하지 않습니다. 즉, 모집단 회귀선은 이론적인 관계를 나타내며, 다양한 표본 데이터로부터 얻은 최소 제곱선은 이론적인 모집단 회귀선과 일정한 패턴을 따르지만 약간의 차이가 있을 수 있다는 것을 보여준다. 임의 변수 Y의 모집단 평균 µ이다. μ는알 수 없지만 Y , y1,...,yn에서 n개의 관측값에 접근할 수 있다.이는 µ를 추정하는 데 사용할 수 있다. 합리적인 추정치는 µˆ = ¯y ![Image](./39.png) 이것은 표본의 평균으로 모든 관측값의 합을 n으로 나눈 값이다. 일반적으로 표본 평균은 모집단 평균을 잘 예측해준다. 선형 회귀 분석에서, "β₀"와 "β₁"은 모집단 회귀선을 정의하는 알 수 없는 계수이다. 이러한 알 수 없는 계수를 "βˆ₀"와 "βˆ₁"로 추정하려고 한다. 이러한 계수 추정치는 최소 제곱선을 정의한다. 최소 제곱선은 관측된 데이터를 기반으로 "X"와 "Y" 사이의 관계를 나타낸다. 이 때, 최소 제곱선은 일반적으로 모집단 회귀선과 다를 수 있지만 모집단 회귀선을 잘 근사하는 것으로 일반적으로 간주된다.  
선형 회귀와 확률 변수 평균 추정 간의 유사성은 편향(bias) 개념에 기반한다. 표본 평균 "µˆ"를 사용하여 "µ"를 추정할 때, 이 추정치는 편향되지 않은(unbiased) 추정치이다. 즉, 평균적으로 "µˆ"가 "µ"와 동일하게 나오기를 기대합니다. 이것이 정확히 무엇을 의미하는 것일까요? 이것은 한 세트의 관측값 "y₁, ..., yn"을 기반으로 "µˆ"가 "µ"를 과대 추정할 수도 있고 다른 세트의 관측값을 기반으로 "µˆ"이 "µ"를 과소 추정할 수도 있다는 것을 의미한다. 그러나 매우 많은 세트의 관측값에서 얻은 "µ"의 추정치를 평균화할 수 있다면, 이러한 추정치의 평균은 "µ"와 정확히 일치하게 될 것이다. 따라서 편향되지 않은 추정치는 실제 파라미터를 체계적으로 과대 또는 과소 추정하지 않는다. 이 특성은 (3.4)에서 주어진 최소 제곱 계수 추정치에도 해당된다. 특정 데이터 세트를 기반으로 "β₀"와 "β₁"을 추정하면 이러한 추정치는 정확히 "β₀"와 "β₁"과 동일하지 않을 것이다. 그러나 많은 데이터 세트에서 얻은 추정치를 평균화할 수 있다면 이러한 추정치의 평균은 정확할 것이다. 사실, 그림 3.3의 오른쪽 패널에서 보듯이 많은 최소 제곱선의 평균은 마치 실제 모집단 회귀선처럼 보이게 된다. 하나의 "µˆ" 추정치가 얼마나 크게 틀릴 수 있을까? 일반적으로 이 질문에 답하기 위해 "µˆ"의 표준 오차인 "SE(µˆ)"로 표시되는 것을 계산한다. 이에 대한 잘 알려진 공식을 가지고 있다. ![Image](./40.png)(3.7) 여기서 σ(시그마)는 Y의 각 실현 yi의 표준편차이다. 대략적으로 말하면, 표준 오류는 이것이 평균적으로 발생하는 양을 알려준다. 이 편차는 n에 따라 줄어든다. 관측값이 많을수록 작아진다. 표준 오류를 계산하려면 βˆ0 및 βˆ1과 관련하여 다음 공식을 사용한다. ![Image](./41.png) 기본적으로 회귀 분석에서 오차("ε")의 분산을 나타내는 "σ²"에 대한 추정치가 필요하다. 이때, 모든 관측값에서 발생하는 오차들의 분산이 "σ²"로 동일하며 서로 상관관계가 없다고 가정해야 한다. 그러나 실제 데이터에서는 이러한 가정이 항상 성립하지는 않지만, 해당 수식은 여전히 좋은 근사치를 제공한다. 이 수식에서 "SE(βˆ₁)"은 "xᵢ"들이 더 퍼져 있을 때 작아지며, 이 경우에 기울기를 추정하는 데 더 많은 영향력을 가진다는 직관을 나타낸다. 또한 "SE(βˆ₀)"은 "x̄"가 0이라면 (이 경우 "βˆ₀"가 "ȳ"과 같아질 것이므로) "SE(µˆ)"와 동일하게 된다. 일반적으로 "σ²"은 알려져 있지 않지만 데이터에서 추정할 수 있다. 이 "σ"의 추정치를 잔차 표준 오차라고하며 다음과 같이 계산된다. ![Image](./42.png) 여기서 "RSS"는 잔차 제곱 합이며 "n"은 관측값 수를 나타낸다. 표준 오차는 신뢰 구간을 계산하는 데 사용된다. 회귀 분석에서 "β₁"에 대한 95% 신뢰 구간은 대략 다음과 같이 나타난다. ![Image](./43.png) 즉, 구간이 다음과 같을 확률은 대략 95%이다. ![Image](./44.png) β1의 실제 값을 포함한다. 마찬가지로 β0에 대한 신뢰 구간은 다음과 같은 대략적인 형태를 취한다. ![Image](./45.png)  
표준 오류(SE 또는 표준 오류)는 다음과 같은 계수에 대한 가설 검정에도 사용된다.  
 Null hypothesis: ![Image](./46.png) Alternative hypothesis:  ![Image](./47.png) 수학적으로는 테스트에 해당한다.  
H0 : β 1 = 0 대 H1 : β1 != 0  
 "β₁"가 0이면 모델 (3.5)이 "Y = β₀ + ε"로 축소되어 "X"가 "Y"와 관련이 없다는 것을 의미한다. 귀무가설을 검정하려면 다음을 결정해야 한다.
β1에 대한 우리의 추정치인 βˆ1이 0에서 충분히 멀리 떨어져 있는지 여부. 얼마나 먼 것이 충분한 것일까요? 이것은 "βˆ₁"의 정확도, 즉 "SE(βˆ₁)"에 의존한다. 만약 "SE(βˆ₁)"이 작다면, 상대적으로 작은 "βˆ₁" 값들조차도 "β₁ ≠ 0"임을 강력히 입증할 수 있으며, 따라서 "X"와 "Y" 사이에 관계가 있다는 것을 나타낸다. 반면에 "SE(βˆ₁)"이 크다면, 우리는 널(null) 가설을 기각하기 위해 "βˆ₁"의 절대값이 큰 값을 가져야 한다. 실제로는 다음과 같이 계산된 t-통계량을 사용하여 검정한다. ![Image](./48.png) 이것은 "βˆ₁"이 0에서 얼마나 떨어져 있는지를 나타내며, 이것이 충분히 크다면 "X"와 "Y" 간의 관계가 존재한다고 결론지을 수 있다. 실제로 X와 Y 사이에 관계가 없으면 t 값은 n - 2 자유도를 갖는 스튜던트 t 분포를 갖는다. 그런 다음 |t|와 같은 숫자를 관찰할 확률을 계산한다. 또는 β1 = 0이라고 가정하면 절대값이 더 크다. 우리는 이 확률 p-값이라고 부른다. 간단히 말해서, 우리는 p-값을 예측 변수와 반응 사이에 실제 연관성이 없는 경우 우연으로 인해 예측 변수와 반응 간의 중요한 연관성을 관찰할 가능성이 없다는 표시로 해석할 수 있다. 따라서 작은 p-값을 관찰하면 예측 변수와 반응 사이에 연관성이 있다고 추론할 수 있다. 따라서 우리는 귀무가설을 기각하고 p-값이 충분히 작을 때 X와 Y 사이에 관계가 있다고 명시한다. 귀무 가설을 기각하기 위한 일반적인 p-값 구분은 5% 또는 1%이다. 귀무가설의 기각이 확인되면 모델의 정확성을 알아야 하며 이는 R²(R-Squared)와 잔차 표준 오차(잔차 표준 오차 또는 RSE(Relative standard error))를 통해 수행된다. RSE는 모델 오류의 표준 편차에 대한 추정치이며, 실제 회귀선에서 응답이 만들어내는 평균 편차이다. 따라서 모델이 얼마나 조정되지 않았는지 절대값으로 측정하고, 값이 낮을수록 예측이 실제 값에 가까워진다. ![Image](./49.png) R² 통계는 모형 적합성을 측정하는 대체 방법이다. 이는 모델에 의해 설명되는 분산의 비율이며 0과 1 사이이다. R²를 계산하려면 다음 공식을 사용한다. ![Image](./50.png) TSS는 총 제곱합이며 ![Image](./51.png) Y에 대한 응답의 분산을 측정한다. 이는 회귀 분석이 수행되기 전 응답에 내재된 변동성의 양으로 정의할 수 있다. RSS는 회귀 성능으로 설명되지 않는 변동성의 양을 측정하므로 TSS — RSS에는 모델 성능으로 설명되는 변동성이 있다. R²가 1에 가까울수록 Y의 변동성 비율이 X를 사용하여 더 많이 설명된다. 





























































